29/09/2025

Qwen3-Coder-30B-A3B-Instruct-GGUF
•  Location: /home/jcanossa/.lmstudio/models/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF/
•  File: Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf
•  Size: 18 GB
•  Type: Q4_K_M quantized GGUF format
•  Purpose: Code generation and instruction-following model

A 30B model, even when heavily quantized (shrunk down), is far too large to fit into your GPU's VRAM.
Model VRAM Required (approximate):
Smallest quantization (Q2_K): ~14 GB
Medium quantization (Q4_K_M): ~18 GB

My GPU VRAM Available:
AMD Radeon RX 570: 4 GB or 8 GB
Since you can't offload the whole model to the GPU, llama.cpp will be forced to run the vast majority of the model's layers on your computer's main CPU.
Massive CPU Load: A 30-billion parameter model is an immense computational task for a CPU. It will be working incredibly hard, causing the process to crawl.
Slow Data Transfer: The few layers you can offload to the GPU will be bottlenecked by the constant, slow data transfer between your system's RAM and the GPU's VRAM.
Expect a generation speed of far less than one token per second. This means waiting several seconds for a single word to appear, making any real-time coding assistance impossible.

So the new approach is to use API instead of local inference with llama.cpp or vLLM
